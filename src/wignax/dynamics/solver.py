import jax
import jax.numpy as jnp
import diffrax
from tqdm.auto import tqdm
from typing import Callable, Tuple, Any, Optional

from wignax.core.state import PhaseSpaceState

Time = float
DriftFunc = Callable[[Time, Any, Any], Any]  # f(t, y, args) -> dy/dt

def solve(
    drift_func: DriftFunc,
    initial_state: PhaseSpaceState,
    t_eval: jax.Array,
    args: Any = None,
    batch_size: int = 1024,
) -> Tuple[jnp.ndarray, jnp.ndarray]:
    """
    Solves the Closed System TWA equations (deterministic ODEs) in memory-efficient batches.

    Instead of storing every single trajectory (which consumes massive RAM),
    this solver accumulates the Mean and Variance of the phase-space variables
    on-the-fly.

    Args:
        drift_func: The deterministic EOM (drift). Generated by wignax.utils.symplectic.
                    Signature: f(t, state, args) -> d_state/dt.
        initial_state: The PhaseSpaceState object containing ALL initial samples.
        t_eval: 1D Array of time points to save.
        args: Static arguments (parameters) to pass to the Hamiltonian/Drift.
        batch_size: Number of trajectories to simulate in parallel on the GPU.
                    Adjust this based on your VRAM (e.g., 1024 to 10000).

    Returns:
        mean_traj: Array of shape (len(t_eval), ...). The average of all paths.
        var_traj: Array of shape (len(t_eval), ...). The variance of all paths.
    """
    
    # 1. Configure the Diffrax Solver
    # We choose Dopri5 (Dormand-Prince 5(4)) as a robust default for ODEs.
    # It has adaptive step size control, which is excellent for stiff Hamiltonians.
    solver_algorithm = diffrax.Dopri5()
    
    # Step size controller (PID controller for adaptive steps)
    # rtol/atol set to standard scientific precision
    stepsize_controller = diffrax.PIDController(rtol=1e-5, atol=1e-6)

    # 2. Define the Single Batch Runner (JIT Compiled)
    # This function runs ONE batch of trajectories and returns their sum/sq_sum.
    # It is defined inside the closure to capture 'drift_func' and 't_eval'.
    
    @jax.jit
    def run_single_batch(state_batch_samples):
        """
        Runs the simulation for a subset (batch) of trajectories.
        """
        # A. Define the physics for one single particle
        def solve_one_trajectory(y0):
            # Define the ODE term: dy/dt = drift(t, y, args)
            term = diffrax.ODETerm(drift_func)
            
            # Solve using Diffrax
            sol = diffrax.diffeqsolve(
                term,
                solver_algorithm,
                t0=t_eval[0],
                t1=t_eval[-1],
                dt0=0.01,  # Initial step size guess
                y0=y0,
                args=args,
                saveat=diffrax.SaveAt(ts=t_eval), # Save only at requested times
                stepsize_controller=stepsize_controller,
                max_steps=100000 # Safety limit to prevent infinite loops
            )
            return sol.ys

        # B. Vectorize over the batch
        # vmap allows us to solve 'batch_size' ODEs in parallel efficiently
        batch_results = jax.vmap(solve_one_trajectory)(state_batch_samples)

        # C. Reduce Data (Online Statistics)
        # We sum immediately to collapse the 'batch' dimension.
        # This prevents returning a massive (batch, time, ...) array.
        batch_sum = jnp.sum(batch_results, axis=0)
        
        # For variance calculation: Sum of squares.
        # jnp.abs() is crucial for complex numbers (bosons). 
        # For real numbers (spins/quadratures), abs() does nothing harmful.
        batch_sq_sum = jnp.sum(jnp.abs(batch_results)**2, axis=0)
        
        return batch_sum, batch_sq_sum

    # 3. The Execution Loop (Batching)
    n_total = initial_state.n_samples
    num_batches = int(jnp.ceil(n_total / batch_size))
    
    # A. Initialize accumulators
    # We infer the shape and dtype from the first sample in the initial state
    sample_shape = jax.eval_shape(lambda: initial_state.samples[0])
    acc_shape = (len(t_eval),) + sample_shape.shape
    dtype = sample_shape.dtype
    
    total_sum = jnp.zeros(acc_shape, dtype=dtype)
    total_sq_sum = jnp.zeros(acc_shape, dtype=jnp.float32) # Variance is always real

    print(f"Wignax Simulation: {n_total} trajectories | {num_batches} batches")

    # B. Main Loop (CPU side driving GPU)
    for i in tqdm(range(num_batches), desc="Simulating"):
        # Slice the data for this batch
        start_idx = i * batch_size
        end_idx = min((i + 1) * batch_size, n_total)
        actual_batch_size = end_idx - start_idx
        
        # Dynamic slice is JAX-friendly way to grab a chunk
        batch_samples = jax.lax.dynamic_slice_in_dim(
            initial_state.samples, start_idx, actual_batch_size
        )

        # Run Batch
        b_sum, b_sq_sum = run_single_batch(batch_samples)
        
        # Accumulate results
        total_sum = total_sum + b_sum
        total_sq_sum = total_sq_sum + b_sq_sum

    # 4. Finalize Statistics
    mean_traj = total_sum / n_total
    
    # Variance formula: E[x^2] - |E[x]|^2
    # Note: For complex numbers, this gives the variance of the magnitude (mostly).
    # Ideally, for full quantum tomography, one tracks covariance matrices, 
    # but this is the standard "Variance" output for TWA visualization.
    var_traj = (total_sq_sum / n_total) - jnp.abs(mean_traj)**2
    
    return mean_traj, var_traj